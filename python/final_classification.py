''' set up '''
# Common imports
import numpy as np
import matplotlib.pyplot as plt
from scipy import io
import pickle
import pandas as pd
import os
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import Perceptron
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
import tensorflow as tf

''' Load Training Data '''
data = io.loadmat('C:/Users/wuh00/OneDrive/Desktop/SPRING_2019/ECE595_ML/Project_1/train_data.mat')
all_images_normalized = data['all_images_normalized']
label = data['label']

'''confusion matrix'''
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'
    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)    
    accuracy = np.trace(cm) / float(np.sum(cm)) * 100
    
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    fig, ax = plt.subplots(figsize=(6, 6))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel=('Predicted label\nnaccuracy={:0.1f}%'.format(accuracy)))
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    # Loop over data dimensions and create text annotations.
    fmt = '.1f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

''' determine PCA components '''
# Fitting the PCA algorithm with our Data
pca = PCA().fit(all_images_normalized)
# Plotting the Cumulative Summation of the Explained Variance
plt.figure()
plt.xlim(right=210)  # adjust the right leaving left unchanged
plt. xlim(left=0)  # adjust the left leaving right unchanged
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') # for each component
plt.title('Pulsar Dataset Explained Variance')
plt.show()    

class_name = unique_labels(label)
print(class_name)

''' KNN & SVM '''
# 1.we use entire X and y
X_KNN_SVM = all_images_normalized
y_KNN_SVM = label

# 2.PCA transformation for KNN and SVM
pca = PCA(n_components = 200)
X_KNN_SVM_PCA = pca.fit_transform(X_KNN_SVM)

# 3a. set up KNN model
knn_clf = KNeighborsClassifier(n_neighbors=1,metric='euclidean')

# 4a. splitting the dat
# cv=10 for 10 folds 
y_pred_KNN = cross_val_predict(knn_clf, X_KNN_SVM_PCA, y_KNN_SVM.ravel(), cv=10)

np.set_printoptions(precision=2)
# 5a. Plot non-normalized confusion matrix
print('--------------KNN-------------------')
plot_confusion_matrix(y_KNN_SVM.ravel(), y_pred_KNN, classes=class_name,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plot_confusion_matrix(y_KNN_SVM.ravel(), y_pred_KNN, classes=class_name, normalize=True,
                      title='Normalized confusion matrix')
plt.show()
print('------------------------------------')

# 3b. set up SVM model
svm_clf = SVC(kernel="poly", degree=2, gamma='auto')

# 4b. splitting the dat
# cv=10 for 10 folds 
y_pred_SVM = cross_val_predict(svm_clf, X_KNN_SVM_PCA, y_KNN_SVM.ravel(), cv=10)

np.set_printoptions(precision=2)
# 5b. Plot non-normalized confusion matrix
print('--------------SVM-------------------')
plot_confusion_matrix(y_KNN_SVM.ravel(), y_pred_SVM, classes=class_name,
                      title='Confusion matrix, without normalization')


# Plot normalized confusion matrix
plot_confusion_matrix(y_KNN_SVM.ravel(), y_pred_SVM, classes=class_name, normalize=True,
                      title='Normalized confusion matrix')
plt.show()
print('------------------------------------')

''' Neural Network '''
pca = PCA(n_components = 200)
all_images_normalized = pca.fit_transform(all_images_normalized)

# 1. Training and Validation Datasets 
test_ratio = 0.2
validation_ratio = 0.2
total_size = all_images_normalized.shape[0]

test_size = int(total_size * test_ratio)
validation_size = int(total_size * validation_ratio)
train_size = total_size - test_size - validation_size

np.random.seed(13)
rnd_indices = np.random.permutation(total_size)
X_train = all_images_normalized[rnd_indices[:train_size]]
y_train = label[rnd_indices[:train_size]]
y_train = y_train.reshape((y_train.shape[0],))
X_valid = all_images_normalized[rnd_indices[train_size:-test_size]]
y_valid = label[rnd_indices[train_size:-test_size]]
y_valid = y_valid.reshape((y_valid.shape[0],))
X_test = all_images_normalized[rnd_indices[-test_size:]]
y_test = label[rnd_indices[-test_size:]]
y_test = y_test.reshape((y_test.shape[0],))

n_inputs = X_train.shape[1] 
n_hidden1 = round(0.66 * X_train.shape[1]) + 10
n_outputs = 10

tf.reset_default_graph()
X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
y = tf.placeholder(tf.int32, shape=(None), name="y")

with tf.name_scope("dnn"):
    hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1",
                              activation=tf.nn.sigmoid)
    logits = tf.layers.dense(hidden1, n_outputs, name="outputs")
    y_proba = tf.nn.sigmoid(logits)

with tf.name_scope("loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss = tf.reduce_mean(xentropy, name="loss")
    
    loss_summary = tf.summary.scalar('log_loss', loss)

learning_rate = 0.01
with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    training_op = optimizer.minimize(loss)

with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  
    accuracy_summary = tf.summary.scalar('accuracy', accuracy)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

from datetime import datetime
def log_dir(prefix=""):
    now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    root_logdir = "tf_logs"
    if prefix:
        prefix += "-"
    name = prefix + "run-" + now
    return "{}/{}/".format(root_logdir, name)

logdir = log_dir("mnist_dnn")
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
m, n = X_train.shape
n_epochs = 400
batch_size = 50
n_batches = int(np.ceil(m / batch_size))

checkpoint_path = "/tmp/my_model.ckpt"
checkpoint_epoch_path = checkpoint_path + ".epoch"
final_model_path = "./my_model"

best_loss = np.infty
epochs_without_progress = 0
max_epochs_without_progress = 50

with tf.Session() as sess:
    if os.path.isfile(checkpoint_epoch_path):
        # if the checkpoint file exists, restore the model and load the epoch number
        with open(checkpoint_epoch_path, "rb") as f:
            start_epoch = int(f.read())
        print("Training was interrupted. Continuing at epoch", start_epoch)
        saver.restore(sess, checkpoint_path)
    else:
        start_epoch = 0
        sess.run(init)

    for epoch in range(start_epoch, n_epochs):
        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})
        file_writer.add_summary(accuracy_summary_str, epoch)
        file_writer.add_summary(loss_summary_str, epoch)
        if epoch % 5 == 0:
            print("Epoch:", epoch,
                  "\tValidation accuracy: {:.3f}%".format(accuracy_val * 100),
                  "\tLoss: {:.5f}".format(loss_val))
            saver.save(sess, checkpoint_path)
            with open(checkpoint_epoch_path, "wb") as f:
                f.write(b"%d" % (epoch + 1))
            if loss_val < best_loss:
                saver.save(sess, final_model_path)
                best_loss = loss_val
            else:
                epochs_without_progress += 5
                if epochs_without_progress > max_epochs_without_progress:
                    print("Early stopping")
                    break

os.remove(checkpoint_epoch_path)

with tf.Session() as sess:
    saver.restore(sess, final_model_path) # or better, use save_path
#     X_new_scaled = X_test[:20]
    X_new_scaled = X_test
    Z = logits.eval(feed_dict={X: X_new_scaled})
    y_pred = np.argmax(Z, axis=1)
    print(np.array(accuracy_summary_str))

print("Predicted classes:", y_pred[:20])
print("Actual classes:   ", y_test[:20])

np.set_printoptions(precision=2)

print('--------------ANN-------------------')
plot_confusion_matrix(y_test, y_pred, classes=class_name,
                      title='Confusion matrix, without normalization')


# Plot normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=class_name, normalize=True,
                      title='Normalized confusion matrix')

plt.show()
print('------------------------------------')